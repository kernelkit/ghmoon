#!/usr/bin/env python3

import argparse
import getpass
import json
import os
import socket
import subprocess
import sys
import time
import traceback
import yaml

try:
    DEFAULT_REPORT=open(os.path.join(os.path.dirname(os.path.realpath(__file__)), "report.sh")).read()
except:
    sys.stderr.write("WARNING: No default report script found\n")
    DEFAULT_REPORT=None

MIN_ENQUEUE_INTERVAL = 30

gistgh = None
context = None
config = None
repos = {}
wq = None

class GH:
    def __init__(self, repo="{owner}/{repo}", path=None, token=None):
        self.path, self.repo = path, repo

        self.env = os.environ.copy()
        if token:
            self.env["GH_TOKEN"] = token

    def exec(self, *args, **kwargs):
        args = list(args)

        args[0] = ["gh"] + args[0]

        if self.path and "cwd" not in kwargs:
            kwargs["cwd"] = self.path

        if "check" not in kwargs:
            kwargs["check"] = True

        kwargs["env"] = self.env | kwargs.get("env", {})

        return subprocess.run(*args, **kwargs)

    def api(self, endpoint, data={}, method=None):
        if not method:
            method = "POST" if data else "GET"

        fields = (("--field", f"{key}={val}") for key, val in data.items())
        opts = [e for pair in fields for e in pair]
        opts += ["--method", method]

        result = self.exec(["api", endpoint] + opts, stdout=subprocess.PIPE)
        return json.loads(result.stdout)

    def gist(self, contents, name="README.md"):
        proc = self.exec(["gist", "create", "-f", name, "-"], text=True,
                         input=contents, stdout=subprocess.PIPE)
        return proc.stdout


class Git:
    def __init__(self, path, origin):
        self.path, self.origin = path, origin
        if not os.path.exists(self.path):
            self.fetch()

    def exec(self, *args, **kwargs):
        args = list(args)

        args[0] = ["git"] + args[0]

        if self.path and "cwd" not in kwargs:
            kwargs["cwd"] = self.path

        if "check" not in kwargs:
            kwargs["check"] = True

        return subprocess.run(*args, **kwargs)

    def fetch(self):
        if not os.path.exists(self.path):
            os.makedirs(self.path)
            self.exec(["clone", "--recurse-submodules", "-j8", self.origin, "."])
        else:
            self.exec(["fetch", "--recurse-submodules"])

class GHArtifact:
    def __init__(self, repo, data):
        self.repo, self.data = repo, data
        self.sha = self.data["workflow_run"]["head_sha"]
        self.br = self.data["workflow_run"]["head_branch"]

    def is_new(self, context):
        statuses = self.repo.api(f"commits/{self.sha}/statuses")
        return not any(map(lambda status: status["context"] == context, statuses))

class GHCommit:
    def __init__(self, repo, sha):
        self.repo, self.sha = repo, sha
        self.hook_env = os.environ | {
            "CONTEXT": context,
            "REPO": str(repo),
            "SHA": sha,
        }

        for hook in ("report-deploy-summary", "report-test-summary"):
            script = self.repo.config.get("hooks", {}).get(hook)
            if script:
                self.hook_env |= { hook.upper().replace("-", "_"): script }

    def exists(self):
        return self.repo.git.exec(["cat-file", "-e", self.sha],
                                  stderr=subprocess.DEVNULL, check=False).returncode == 0

    def status(self, state, desc, url=None):
        urlstr = f" ({url})" if url else ""
        print(f"{self.repo}@{self.sha:.8s}: [{state}] {desc}{urlstr}")

        fields = { "context": context, "state": state, "description": desc }
        if url:
            fields |= { "target_url": url}
        self.repo.api(f"statuses/{self.sha}", fields)

    def checkout(self):
        self.repo.git.exec(["checkout", "--recurse-submodules", self.sha])

    def exec_hook(self, hook, interactive=True, default=None, **kwargs):
        kwargs["env"] = kwargs.get("env", {}) | self.hook_env

        if interactive:
            log=None
        else:
            logpath = f"{self.repo.path}/{hook}-{self.sha}.log"
            self.hook_env |= { f"{hook.upper()}_LOG": logpath }
            log = open(logpath, "w")

        script = self.repo.config.get("hooks", {}).get(hook)
        if not script:
            if not default:
                assert script, f"Required hook \"{hook}\" not found for {self.repo}"
            script = default

        proc = subprocess.run(["/bin/sh"], input=script, cwd=self.repo.git.path,
                              check=False, text=True,
                              stdout=log, stderr=subprocess.STDOUT, **kwargs)

        self.hook_env |= { f"{hook.upper()}_EXITCODE": str(proc.returncode) }
        if proc.returncode != 0:
            raise Exception(f"{hook} of {self.repo}@{self.sha} failed with code {proc.returncode}")

    def process(self, interactive=True):
        def local_status(state, desc, url=None):
            urlstr = f" ({url})" if url else ""
            print(f"{self.repo}@{self.sha:.8s}: [{state}] {desc}{urlstr}")

        status = local_status if interactive else self.status

        status("pending", "Deploying Artifact")
        try:
            state = "error"
            self.exec_hook("deploy", interactive)
            status("pending", "Running tests")
            state = "failure"
            self.exec_hook("test", interactive)
            state = "success"
        except Exception as e:
            sys.stderr.write(f"EXCEPTION: {str(e)}\n")
            traceback.print_exc()
            pass

        gist = None
        try:
            self.exec_hook("report", interactive, default=DEFAULT_REPORT)
            if interactive:
                gist = None
            else:
                global gistgh
                gistname = f"{context}-{str(self.repo)}@{self.sha:.8s}.md".replace("/", "-")
                gist = gistgh.gist(open(f"{self.repo.path}/report-{self.sha}.log").read(), gistname)

        except Exception as e:
            sys.stderr.write(f"EXCEPTION: {str(e)}\n")
            traceback.print_exc()
            pass

        try:
            self.exec_hook("cleanup", interactive, default="true")
        except Exception as e:
            sys.stderr.write(f"EXCEPTION: {str(e)}\n")
            traceback.print_exc()
            pass

        if state == "error":
            status("error", "Failed to deploy artifact", gist)
        elif state == "failure":
            status("failure", "Test failed", gist)
        elif state == "success":
            status("success", "Test passed", gist)
        else:
            status("error", "Internal error", gist)

        return state == "success"


class GHRepo:
    def __init__(self, repo: str, config: dict):
        self.repo, self.config = repo, config
        self.path = config.get("worktree", os.path.expanduser(f"~/.ghmoon/{repo}"))
        os.makedirs(self.path, exist_ok=True)
        self.git = Git(f"{self.path}/git", f"https://github.com/{self.repo}.git") # TODO pass token to private repos
        self.gh = GH(repo=self.repo, path=f"{self.path}/git", token=config.get("token"))

    def __str__(self):
        return self.repo

    def api(self, endpoint, data={}, method=None):
        return self.gh.api(f"/repos/{self.repo}/{endpoint}", data, method)

    def get_matching_artifacts(self):
        arts = self.api("actions/artifacts").get("artifacts", [])

        jq = subprocess.run(["jq", "[.[] | select(" + self.config["match"] + ")]"],
                            input=json.dumps(arts), stdout=subprocess.PIPE,
                            text=True, check=True)
        matches = json.loads(jq.stdout)
        return [GHArtifact(self, data) for data in matches]

    def process(self, sha, interactive=True):
        c = GHCommit(self, sha)
        if not c.exists():
            self.git.fetch()

        c.checkout()

        if self.config.get("git", {}).get("clean", True):
            self.git.exec(["clean", "-ffdx"])

        return c.process(interactive)


class WorkQueue:
    def __init__(self, path, manage=False, publish=False):
        self.publish = publish
        self.paths = {
            "todo": os.path.join(path, "todo"),
            "doing": os.path.join(path, "doing"),
            "done": os.path.join(path, "done"),
        }

        for path in self.paths.values():
            os.makedirs(path, exist_ok=True)

        if manage:
            for job in os.listdir(self.paths["doing"]):
                sys.stderr.write(f"Rescheduling unfinished job {job}\n")
                os.rename(os.path.join(self.paths["doing"], job),
                          os.path.join(self.paths["todo"], job))

    def _job_paths(self, name):
        return {
            state: os.path.join(self.paths[state], f"{name}.json")
            for state in self.paths.keys()
        }

    def is_known(self, name):
        paths = self._job_paths(name)
        return any(map(os.path.exists, paths.values()))

    def _enqueue(self, name, data):
        data |= { "name": name }
        paths = self._job_paths(name)

        try:
            fd = os.open(paths["todo"], os.O_CREAT | os.O_EXCL | os.O_WRONLY, 0o644)
        except FileExistsError:
            sys.stderr.write(f"Ignoring {name}: already queued\n")
            return False

        if os.path.exists(paths["doing"]):
            os.close(fd)
            os.remove(paths["todo"])
            sys.stderr.write(f"Ignoring {name}: already in progress\n")
            return False
        elif os.path.exists(paths["done"]):
            os.close(fd)
            os.remove(paths["todo"])
            sys.stderr.write(f"Ignoring {name}: already processed\n")
            return False

        with os.fdopen(fd, "w") as f:
            f.write(json.dumps(data))

        sys.stderr.write(f"Queued {name}\n")
        return True

    def _enqueue_repo(self, repo):
        sys.stderr.write(f"Polling {str(repo)}\n")

        for artifact in repo.get_matching_artifacts():
            name = f"{str(repo).replace('/', '-')}-{artifact.sha}"
            if self.is_known(name):
                continue

            self._enqueue(name, {
                "type": "process",
                "repo": str(repo),
                "sha": artifact.sha,

                "artifact": artifact.data,
            })

    def enqueue(self):
        global repos

        for repo in repos.values():
            self._enqueue_repo(repo)

    def listdir(self, path):
        out = subprocess.check_output(["ls", "-rt", "--time=birth", path], text=True)
        return out.splitlines()

    def _dequeue(self):
        assert not os.listdir(self.paths["doing"]), "Another job is alredy in progress"

        abspaths = [os.path.join(self.paths["todo"], job)
                    for job in self.listdir(self.paths["todo"])]
        for abspath in abspaths:
            try:
                with open(abspath) as f:
                    job = json.loads(f.read())
            except:
                sys.stderr.write(f"Skipping {abspath}: Not a JSON document\n")
                traceback.print_exc()
                continue

            if not job.get("name"):
                sys.stderr.write(f"Skipping {abspath}: Malformed job (no name)\n")
                continue
            elif not job.get("type"):
                sys.stderr.write(f"Skipping {abspath}: Malformed job (no type)\n")
                continue
            elif job["type"] not in ("process"):
                sys.stderr.write(f"Skipping {abspath}: Unsupported type {job['type']}\n")
                continue

            return job

        return None

    def _process_job(self, job):
        name = job["name"]
        paths = self._job_paths(name)

        if not ("repo" in job and "sha" in job):
            sys.stderr.write(f"Skipping {name}: Malformed process job\n")
            os.rename(paths["todo"], paths["done"])
            return
        if not job["repo"] in repos:
            sys.stderr.write(f"Skipping {name}: Unknown repo {job['repo']}\n")
            os.rename(paths["todo"], paths["done"])
            return

        try:
            os.rename(paths["todo"], paths["doing"])
        except:
            sys.stderr.write(f"Skipping {name}: Job disappeared\n")
            return

        try:
            sys.stderr.write(f"Processing {name}\n")
            repos[job["repo"]].process(job["sha"], not self.publish)
            sys.stderr.write(f"Processed {name}\n")
        except Exception as e:
            sys.stderr.write(f"Aborting {abspath}: {str(e)}\n")
            traceback.print_exc()

        os.rename(paths["doing"], paths["done"])
        return

    def process_next(self):
        job = self._dequeue()
        if not job:
            return False

        if job["type"] == "process":
            self._process_job(job)

        return True

    def daemon(self):
        while True:
            self.enqueue()

            t0 = time.time()
            while self.process_next():
                pass
            t = time.time() - t0

            # If the queue is empty, or if something goes wrong, make
            # sure we rate-limit the polling of new artifacts.
            if t < MIN_ENQUEUE_INTERVAL:
                time.sleep(MIN_ENQUEUE_INTERVAL - t)


def init(args):
    global gistgh
    global context
    global config
    global repos
    global wq

    cfgfile = args.config
    if not cfgfile:
        for src in (os.path.expanduser("~/.ghmoon/config.yaml"),
                    "/etc/ghmoon/config.yaml"):
            try:
                cfgfile = open(src, "r")
                break
            except:
                pass

        assert cfgfile, "No config specified, an no default found"

    config = yaml.load(cfgfile, Loader=yaml.FullLoader)
    context = config.get("context", f"{getpass.getuser()}@{socket.gethostname()}")

    gistgh = GH(token=config.get("gist", {}).get("token"))

    for repo in config.get("repos"):
        repos[repo] = GHRepo(repo, config["repos"][repo])

    wq = WorkQueue(os.path.expanduser(f"~/.ghmoon/workqueue"),
                   "cmd" in args and args.cmd == "daemon",
                   "publish" in args and args.publish)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="ghmoon")
    parser.add_argument("-f", "--config-file",
                        type=argparse.FileType("r"), dest="config")

    sps = parser.add_subparsers(dest="cmd")

    dae_sp = sps.add_parser("daemon")
    dae_sp.add_argument("-p", "--publish", action="store_true")

    enq_sp = sps.add_parser("enqueue")

    pro_sp = sps.add_parser("process")
    pro_sp.add_argument("-p", "--publish", action="store_true")
    pro_sp.add_argument("repo", type=str)
    pro_sp.add_argument("sha", type=str)

    args = parser.parse_args()
    init(args)

    if "repo" in args and args.repo:
        assert args.repo in repos, f"\"{args.repo}\" is not a repo we orbit"

    if False:
        pass
    elif args.cmd == "daemon":
        wq.daemon()
    elif args.cmd == "enqueue":
        wq.enqueue()
    elif args.cmd == "process":
        sys.exit(0 if repos[args.repo].process(args.sha, not args.publish) else 1)
